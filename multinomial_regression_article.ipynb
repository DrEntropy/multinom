{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Regression in PyMC: From Basics to Random Effects\n",
    "\n",
    "This article covers multinomial regression modeling in PyMC, from simple classification problems to hierarchical models with correlated random effects. We'll explore both the high-level Bambi interface and the low-level PyMC approach, discussing when each is appropriate.\n",
    "\n",
    "**Prerequisites:** Familiarity with Bayesian basics (priors, posteriors, MCMC) and comfort with Python and basic PyMC usage.\n",
    "\n",
    "## What is Multinomial Regression?\n",
    "\n",
    "Multinomial regression extends binary logistic regression to handle outcomes with more than two categories. While logistic regression models the log-odds of a single event, multinomial regression models the log-odds of each category relative to a reference category:\n",
    "\n",
    "$$\\log\\left(\\frac{P(Y=k)}{P(Y=K)}\\right) = X\\beta_k$$\n",
    "\n",
    "where $K$ is the reference category and $k$ ranges over the other categories.\n",
    "\n",
    "**Common applications include:**\n",
    "- Classification with multiple classes (e.g., species identification)\n",
    "- Choice modeling (e.g., consumer product selection)\n",
    "- Compositional data analysis (e.g., vote shares, market shares)\n",
    "\n",
    "## The Identifiability Problem\n",
    "\n",
    "A key challenge in multinomial regression is identifiability. The model is overparameterized: adding a constant to all category logits doesn't change the probabilities. Two common solutions exist:\n",
    "\n",
    "1. **Reference category (pivot) approach:** Fix one category's coefficients to zero\n",
    "2. **Sum-to-zero constraint:** Constrain parameters to sum to zero across categories\n",
    "\n",
    "We'll demonstrate both approaches, with emphasis on PyMC's `ZeroSumNormal` distribution which implements the second approach elegantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import bambi as bmb\n",
    "import pytensor.tensor as pt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import softmax\n",
    "import sklearn.datasets\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyMC version: {pm.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Multinomial Regression with the Iris Dataset\n",
    "\n",
    "We'll start with the classic Iris dataset, predicting species from sepal and petal measurements. This is an ideal example because:\n",
    "- It has exactly 3 categories (setosa, versicolor, virginica)\n",
    "- The features have clear predictive power\n",
    "- The data is well-behaved for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "# Center the predictors (good practice for interpretation)\n",
    "for col in ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']:\n",
    "    iris[col] = iris[col] - iris[col].mean()\n",
    "\n",
    "print(f\"Dataset shape: {iris.shape}\")\n",
    "print(f\"Species: {iris['species'].unique()}\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "sns.pairplot(iris, hue=\"species\", diag_kind=\"kde\")\n",
    "plt.suptitle(\"Iris Dataset: Feature Relationships by Species\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The Bambi Approach (High-Level)\n",
    "\n",
    "Bambi provides a formula-based interface similar to R's brms. For multinomial regression, we use the `categorical` family. Bambi automatically handles the reference category approach, treating the first category (alphabetically) as the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bambi model - one line!\n",
    "bambi_model = bmb.Model(\n",
    "    \"species ~ sepal_length + sepal_width + petal_length + petal_width\", \n",
    "    iris, \n",
    "    family=\"categorical\",\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "bambi_idata = bambi_model.fit(random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the summary\n",
    "az.summary(bambi_idata, round_to=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the coefficients:**\n",
    "\n",
    "Bambi uses 'setosa' as the reference category. Each coefficient represents the log-odds of that category relative to setosa, per unit increase in the predictor. For example:\n",
    "- `petal_length[virginica]` ~ 3.7 means that a 1cm increase in petal length increases the log-odds of virginica vs. setosa by about 3.7\n",
    "- Negative coefficients for `sepal_width` indicate that wider sepals favor setosa over the other species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The PyMC Approach with ZeroSumNormal\n",
    "\n",
    "While Bambi's reference category approach works well, PyMC offers `ZeroSumNormal` - a more elegant solution that treats all categories symmetrically. Instead of fixing one category to zero, we constrain all parameters to sum to zero.\n",
    "\n",
    "**Advantages of ZeroSumNormal:**\n",
    "- Symmetric treatment of all categories\n",
    "- Better interpretability (each coefficient represents deviation from the mean)\n",
    "- Improved sampling efficiency in some cases\n",
    "- More natural for compositional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyMC\n",
    "X = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values\n",
    "y = pd.Categorical(iris['species']).codes  # Convert to 0, 1, 2\n",
    "\n",
    "n_obs = X.shape[0]\n",
    "n_features = X.shape[1]\n",
    "n_categories = 3\n",
    "\n",
    "print(f\"Observations: {n_obs}, Features: {n_features}, Categories: {n_categories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyMC model with ZeroSumNormal\n",
    "with pm.Model() as pymc_zsn_model:\n",
    "    # Data containers\n",
    "    X_data = pm.Data('X_data', X)\n",
    "    y_obs = pm.Data('y_obs', y)\n",
    "    \n",
    "    # Intercepts with sum-to-zero constraint\n",
    "    alpha = pm.ZeroSumNormal('alpha', sigma=5, shape=n_categories)\n",
    "    \n",
    "    # Coefficients with sum-to-zero constraint across categories\n",
    "    beta = pm.ZeroSumNormal('beta', sigma=5, shape=(n_features, n_categories))\n",
    "    \n",
    "    # Linear predictor\n",
    "    mu = pm.math.dot(X_data, beta) + alpha\n",
    "    \n",
    "    # Likelihood (logit_p handles the softmax internally)\n",
    "    obs = pm.Categorical('obs', logit_p=mu, observed=y_obs)\n",
    "    \n",
    "    # Sample\n",
    "    pymc_zsn_idata = pm.sample(random_seed=42, idata_kwargs={\"log_likelihood\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(pymc_zsn_idata, round_to=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on ZeroSumNormal interpretation:** With the sum-to-zero constraint, each coefficient represents the category's deviation from the mean effect. A positive `beta[2, 2]` (petal_length for virginica) means virginica has an above-average response to petal length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Comparison: Pivot vs ZeroSumNormal\n",
    "\n",
    "For completeness, let's also build the traditional pivot model to compare sampling efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional pivot model (category 0 as reference)\n",
    "with pm.Model() as pymc_pivot_model:\n",
    "    X_data = pm.Data('X_data', X)\n",
    "    y_obs = pm.Data('y_obs', y)\n",
    "    \n",
    "    # Parameters only for non-reference categories\n",
    "    alpha = pm.Normal('alpha', mu=0, sigma=5, shape=n_categories - 1)\n",
    "    beta = pm.Normal('beta', mu=0, sigma=5, shape=(n_features, n_categories - 1))\n",
    "    \n",
    "    # Compute logits for non-pivot categories\n",
    "    mu_nonpivot = pm.math.dot(X_data, beta) + alpha\n",
    "    \n",
    "    # Add zeros for the pivot category\n",
    "    zeros = pm.math.zeros((n_obs, 1))\n",
    "    mu = pm.math.concatenate([zeros, mu_nonpivot], axis=1)\n",
    "    \n",
    "    obs = pm.Categorical('obs', logit_p=mu, observed=y_obs)\n",
    "    \n",
    "    pymc_pivot_idata = pm.sample(random_seed=42, idata_kwargs={\"log_likelihood\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ESS between models\n",
    "zsn_summary = az.summary(pymc_zsn_idata)\n",
    "pivot_summary = az.summary(pymc_pivot_idata)\n",
    "\n",
    "print(\"ZeroSumNormal model - Mean ESS bulk:\", zsn_summary['ess_bulk'].mean().round(0))\n",
    "print(\"Pivot model - Mean ESS bulk:\", pivot_summary['ess_bulk'].mean().round(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Model Diagnostics\n",
    "\n",
    "Before trusting our results, we should check sampling diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plots for the ZeroSumNormal model\n",
    "az.plot_trace(pymc_zsn_idata, var_names=['alpha', 'beta'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check R-hat and divergences\n",
    "summary = az.summary(pymc_zsn_idata)\n",
    "print(f\"Max R-hat: {summary['r_hat'].max():.3f}\")\n",
    "print(f\"Min ESS bulk: {summary['ess_bulk'].min():.0f}\")\n",
    "print(f\"Divergences: {pymc_zsn_idata.sample_stats.diverging.sum().values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive check\n",
    "with pymc_zsn_model:\n",
    "    ppc = pm.sample_posterior_predictive(pymc_zsn_idata, random_seed=42)\n",
    "\n",
    "# Compare predicted vs actual class frequencies\n",
    "pred_classes = ppc.posterior_predictive['obs'].values.flatten()\n",
    "actual_counts = np.bincount(y, minlength=3) / len(y)\n",
    "pred_counts = np.bincount(pred_classes, minlength=3) / len(pred_classes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, actual_counts, width, label='Observed', alpha=0.8)\n",
    "ax.bar(x + width/2, pred_counts, width, label='Predicted', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['setosa', 'versicolor', 'virginica'])\n",
    "ax.set_ylabel('Proportion')\n",
    "ax.set_title('Posterior Predictive Check: Class Frequencies')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Random Effects Multinomial Regression\n",
    "\n",
    "Real-world data often has hierarchical structure: students nested in schools, patients in hospitals, or repeated measurements on individuals. Random effects models account for this grouping, allowing us to:\n",
    "\n",
    "1. **Capture group-level variation** in category preferences\n",
    "2. **Model overdispersion** in count data\n",
    "3. **Estimate correlations** between category preferences within groups\n",
    "\n",
    "### 2.1 Why Random Effects?\n",
    "\n",
    "Consider a study of consumer preferences across multiple stores. Without random effects, we assume all stores have identical category preferences. With random effects, each store can have its own preference pattern, and we can learn about the correlation structure of these preferences.\n",
    "\n",
    "### 2.2 Simulated Data Example\n",
    "\n",
    "We'll generate data with known correlation structure to validate our model's ability to recover true parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation settings\n",
    "np.random.seed(33)\n",
    "num_groups = 100\n",
    "num_categories = 3\n",
    "\n",
    "# True covariance structure for random effects\n",
    "# Using pivot parameterization: only num_categories-1 random effects\n",
    "true_corr = np.array([\n",
    "    [1.0, 0.7],\n",
    "    [0.7, 1.0]\n",
    "])\n",
    "true_sds = np.array([0.5, 1.0])\n",
    "true_cov = np.diag(true_sds) @ true_corr @ np.diag(true_sds)\n",
    "\n",
    "print(\"True covariance matrix:\")\n",
    "print(true_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate group-level random effects\n",
    "mean_effect = np.zeros(num_categories - 1)\n",
    "group_effects = multivariate_normal.rvs(mean=mean_effect, cov=true_cov, size=num_groups)\n",
    "\n",
    "# Add pivot logits (zeros for category 0)\n",
    "group_effects_full = np.hstack([np.zeros((num_groups, 1)), group_effects])\n",
    "\n",
    "# Verify empirical covariance\n",
    "emp_cov = np.cov(group_effects.T)\n",
    "print(\"Empirical covariance (should be close to true):\")\n",
    "print(emp_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate observations\n",
    "data = []\n",
    "for idx in range(num_groups):\n",
    "    n_obs = np.random.poisson(30)  # ~30 observations per group\n",
    "    probs = softmax(group_effects_full[idx])\n",
    "    \n",
    "    for _ in range(n_obs):\n",
    "        observed_cat = np.random.choice(num_categories, p=probs)\n",
    "        data.append({\n",
    "            'group_id': idx,\n",
    "            'cat_id': observed_cat,\n",
    "        })\n",
    "\n",
    "simulated_df = pd.DataFrame(data)\n",
    "print(f\"Total observations: {len(simulated_df)}\")\n",
    "print(f\"Observations per group: {simulated_df.groupby('group_id').size().describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregated version for multinomial likelihood\n",
    "agg_df = (\n",
    "    simulated_df\n",
    "    .groupby(['group_id', 'cat_id'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "agg_df.columns = ['group_id', 'cat_0', 'cat_1', 'cat_2']\n",
    "agg_df['total'] = agg_df[['cat_0', 'cat_1', 'cat_2']].sum(axis=1)\n",
    "\n",
    "print(agg_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Bambi's Limitations\n",
    "\n",
    "Bambi can fit random effects in categorical models, but with an important limitation: it assumes **uncorrelated random effects** across categories. This means it estimates a single variance parameter shared across all category-specific random effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add group as categorical for Bambi\n",
    "simulated_df['group'] = simulated_df['group_id'].astype(str)\n",
    "simulated_df['cat'] = simulated_df['cat_id'].apply(lambda x: f'cat_{x}')\n",
    "\n",
    "# Bambi model with random intercepts\n",
    "bambi_re_model = bmb.Model('cat ~ (1|group)', data=simulated_df, family='categorical')\n",
    "bambi_re_idata = bambi_re_model.fit(random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: only one sigma parameter for all random effects\n",
    "az.summary(bambi_re_idata, var_names=['Intercept', '1|group_sigma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly model **correlated** random effects, we need to use PyMC directly with `LKJCholeskyCov`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Full PyMC Implementation with Correlated Random Effects\n",
    "\n",
    "We'll use the `LKJCholeskyCov` prior to estimate the full covariance matrix of random effects. This prior places a uniform distribution on correlation matrices (when `eta=1`) while allowing us to specify priors on the standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated multinomial model with correlated random effects\n",
    "with pm.Model() as re_model:\n",
    "    # Data\n",
    "    group_idx = pm.Data(\"group_idx\", agg_df[\"group_id\"].values)\n",
    "    count_obs = pm.Data(\"count_obs\", agg_df[[\"cat_0\", \"cat_1\", \"cat_2\"]].values)\n",
    "    total_obs = pm.Data(\"total_obs\", agg_df[\"total\"].values)\n",
    "    \n",
    "    # Prior on standard deviations\n",
    "    sd_dist = pm.HalfStudentT.dist(nu=3, sigma=2)\n",
    "    \n",
    "    # LKJ prior on correlation + standard deviations\n",
    "    chol_cov, corr, stds = pm.LKJCholeskyCov(\n",
    "        \"chol_cov\",\n",
    "        n=num_categories - 1,  # Pivot parameterization\n",
    "        eta=1,  # Uniform on correlations\n",
    "        sd_dist=sd_dist,\n",
    "        compute_corr=True\n",
    "    )\n",
    "    \n",
    "    # Non-centered parameterization for random effects\n",
    "    z = pm.Normal(\"z\", 0, 1, shape=(num_groups, num_categories - 1))\n",
    "    group_effects_m = pm.Deterministic(\n",
    "        \"group_effects\",\n",
    "        pt.dot(z, chol_cov.T)\n",
    "    )\n",
    "    \n",
    "    # Overall mean (could be zero, but we estimate it)\n",
    "    mean_eff = pm.Normal(\"mean_effect\", 0, 1, shape=(num_categories - 1))\n",
    "    \n",
    "    # Logits with pivot at zero\n",
    "    logits = pt.concatenate(\n",
    "        [pt.zeros((num_groups, 1)), group_effects_m + mean_eff],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Multinomial likelihood\n",
    "    p = pm.math.softmax(logits, axis=1)\n",
    "    pm.Multinomial(\"obs\", p=p, n=total_obs, observed=count_obs)\n",
    "    \n",
    "    # Sample with increased target_accept for better exploration\n",
    "    re_trace = pm.sample(\n",
    "        random_seed=42, \n",
    "        target_accept=0.95,\n",
    "        idata_kwargs={\"log_likelihood\": True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check diagnostics\n",
    "az.summary(re_trace, var_names=[\"mean_effect\", \"chol_cov_corr\", \"chol_cov_stds\"], round_to=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the covariance matrix\n",
    "dat = az.extract(re_trace, var_names=[\"chol_cov_corr\", \"chol_cov_stds\"])\n",
    "corr_samples = dat[\"chol_cov_corr\"].values\n",
    "stds_samples = dat[\"chol_cov_stds\"].values[:, None, :]\n",
    "\n",
    "# Compute covariance for each sample\n",
    "cov_samples = corr_samples * stds_samples * stds_samples.transpose(1, 0, 2)\n",
    "estimated_cov = np.mean(cov_samples, axis=2)\n",
    "\n",
    "print(\"True covariance matrix:\")\n",
    "print(true_cov)\n",
    "print(\"\\nEstimated covariance matrix (posterior mean):\")\n",
    "print(estimated_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare true vs estimated random effects\n",
    "true_effects = group_effects  # Shape: (num_groups, 2)\n",
    "posterior_means = re_trace.posterior[\"group_effects\"].mean(dim=[\"chain\", \"draw\"]).values\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].scatter(true_effects[:, i], posterior_means[:, i], alpha=0.6)\n",
    "    lims = [min(true_effects[:, i].min(), posterior_means[:, i].min()) - 0.2,\n",
    "            max(true_effects[:, i].max(), posterior_means[:, i].max()) + 0.2]\n",
    "    axes[i].plot(lims, lims, 'r--', alpha=0.5, label='y=x')\n",
    "    axes[i].set_xlabel(\"True Logits\")\n",
    "    axes[i].set_ylabel(\"Posterior Means\")\n",
    "    axes[i].set_title(f\"Category {i+1} Random Effects\")\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Posterior Predictive Checks\n",
    "\n",
    "Let's verify that our model captures the data-generating process correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate posterior predictions\n",
    "with re_model:\n",
    "    re_ppc = pm.sample_posterior_predictive(re_trace, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predicted vs observed for selected groups\n",
    "selected_groups = [0, 1, 3, 4, 5, 6]\n",
    "num_selected = len(selected_groups)\n",
    "\n",
    "all_pp_counts = az.extract(\n",
    "    re_ppc, group=\"posterior_predictive\", var_names=[\"obs\"]\n",
    ").values  # (num_groups, num_categories, num_samples)\n",
    "\n",
    "pp_counts = all_pp_counts[selected_groups, :, :]\n",
    "obs_counts = agg_df.loc[selected_groups, [\"cat_0\", \"cat_1\", \"cat_2\"]].values\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "categories = [\"cat_0\", \"cat_1\", \"cat_2\"]\n",
    "\n",
    "for idx, group in enumerate(selected_groups):\n",
    "    ax = axes[idx]\n",
    "    data = [pp_counts[idx, cat_idx, :] for cat_idx in range(len(categories))]\n",
    "    sns.boxplot(data=data, ax=ax, color='lightblue')\n",
    "    ax.scatter(range(len(categories)), obs_counts[idx], color='red', s=100, zorder=5, label='Observed')\n",
    "    ax.set_xticks(range(len(categories)))\n",
    "    ax.set_xticklabels(categories)\n",
    "    ax.set_title(f'Group {group}')\n",
    "    ax.set_ylabel('Counts')\n",
    "    if idx == 0:\n",
    "        ax.legend()\n",
    "\n",
    "plt.suptitle('Posterior Predictive Checks: Predicted (boxplots) vs Observed (red dots)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Cross-Validation for Multinomial Models\n",
    "\n",
    "Model comparison is crucial for choosing between alternative specifications. However, standard LOO-CV often fails for multinomial models.\n",
    "\n",
    "### 3.1 Why LOO-PSIS Often Fails\n",
    "\n",
    "Leave-One-Out cross-validation uses Pareto Smoothed Importance Sampling (PSIS) to estimate predictive accuracy. The Pareto k diagnostic indicates the reliability of this approximation:\n",
    "\n",
    "- k < 0.5: Good\n",
    "- 0.5 ≤ k < 0.7: Acceptable\n",
    "- **k ≥ 0.7: Unreliable** - importance sampling fails\n",
    "\n",
    "Multinomial models often have many observations with k > 0.7 because:\n",
    "1. Observations can be highly influential (especially in small groups)\n",
    "2. The categorical likelihood creates discrete \"jumps\" in the posterior\n",
    "3. Group random effects make single observations disproportionately important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt LOO-CV on our random effects model\n",
    "try:\n",
    "    loo_result = az.loo(re_trace)\n",
    "    print(loo_result)\n",
    "    print(f\"\\nPareto k > 0.7: {np.sum(loo_result.pareto_k > 0.7)} observations\")\n",
    "except Exception as e:\n",
    "    print(f\"LOO failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 K-Fold Cross-Validation as Alternative\n",
    "\n",
    "When LOO fails, K-fold CV provides a robust alternative. Instead of approximating leave-one-out, we:\n",
    "1. Split data into K folds\n",
    "2. Refit the model K times, each time holding out one fold\n",
    "3. Evaluate predictive accuracy on held-out folds\n",
    "\n",
    "This is computationally expensive but reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Choosing Folds for Grouped Data (Critical!)\n",
    "\n",
    "**This is a key point that is often overlooked.**\n",
    "\n",
    "For hierarchical models, you must split by **groups**, not individual observations. Why?\n",
    "\n",
    "- If observations from the same group appear in both train and test sets, the random effect for that group is estimated from training data and \"leaks\" into predictions\n",
    "- This dramatically overstates predictive accuracy\n",
    "- The model appears to generalize well, but it's actually memorizing group-specific patterns\n",
    "\n",
    "**Correct approach:** Create folds where each group is entirely in train OR test, never both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kfold_cv_multinomial(model_func, data, n_folds=5, random_seed=42):\n",
    "    \"\"\"\n",
    "    Perform K-fold CV with proper group-level folds.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_func : callable\n",
    "        Function that takes training data and returns (model, trace)\n",
    "    data : DataFrame\n",
    "        Aggregated data with group_id column\n",
    "    n_folds : int\n",
    "        Number of folds\n",
    "    random_seed : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with fold-wise and overall metrics\n",
    "    \"\"\"\n",
    "    # Get unique groups\n",
    "    groups = data['group_id'].unique()\n",
    "    \n",
    "    # Create group-level folds\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "    \n",
    "    fold_results = []\n",
    "    \n",
    "    for fold_idx, (train_groups_idx, test_groups_idx) in enumerate(kf.split(groups)):\n",
    "        train_groups = groups[train_groups_idx]\n",
    "        test_groups = groups[test_groups_idx]\n",
    "        \n",
    "        # Split data by groups\n",
    "        train_data = data[data['group_id'].isin(train_groups)].reset_index(drop=True)\n",
    "        test_data = data[data['group_id'].isin(test_groups)].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\nFold {fold_idx + 1}: {len(train_groups)} train groups, {len(test_groups)} test groups\")\n",
    "        \n",
    "        # Reindex groups for training\n",
    "        group_map = {g: i for i, g in enumerate(train_groups)}\n",
    "        train_data = train_data.copy()\n",
    "        train_data['group_idx'] = train_data['group_id'].map(group_map)\n",
    "        \n",
    "        # Fit model on training data\n",
    "        model, trace = model_func(train_data, len(train_groups))\n",
    "        \n",
    "        # For test data, we need to predict for \"new groups\"\n",
    "        # This means sampling new random effects from the prior\n",
    "        with model:\n",
    "            # Get posterior samples of hyperparameters\n",
    "            mean_eff_samples = trace.posterior['mean_effect'].values\n",
    "            chol_samples = trace.posterior['chol_cov'].values\n",
    "            \n",
    "        # Compute log-likelihood for test groups\n",
    "        # For new groups, we marginalize over the random effects distribution\n",
    "        test_ll = compute_test_loglik(\n",
    "            test_data, mean_eff_samples, chol_samples, n_samples=100\n",
    "        )\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold_idx,\n",
    "            'n_train_groups': len(train_groups),\n",
    "            'n_test_groups': len(test_groups),\n",
    "            'test_elpd': test_ll\n",
    "        })\n",
    "    \n",
    "    return fold_results\n",
    "\n",
    "\n",
    "def compute_test_loglik(test_data, mean_eff_samples, chol_samples, n_samples=100):\n",
    "    \"\"\"\n",
    "    Compute expected log-likelihood for test groups by marginalizing\n",
    "    over the random effects distribution.\n",
    "    \"\"\"\n",
    "    from scipy.stats import multinomial\n",
    "    \n",
    "    n_chains, n_draws, n_cat_minus_1 = mean_eff_samples.shape\n",
    "    total_samples = n_chains * n_draws\n",
    "    \n",
    "    # Flatten chain and draw dimensions\n",
    "    mean_flat = mean_eff_samples.reshape(total_samples, n_cat_minus_1)\n",
    "    chol_flat = chol_samples.reshape(total_samples, -1)\n",
    "    \n",
    "    # Sample indices\n",
    "    sample_idx = np.random.choice(total_samples, size=n_samples, replace=False)\n",
    "    \n",
    "    log_liks = []\n",
    "    \n",
    "    for group_id in test_data['group_id'].unique():\n",
    "        group_data = test_data[test_data['group_id'] == group_id]\n",
    "        counts = group_data[['cat_0', 'cat_1', 'cat_2']].values[0]\n",
    "        total = counts.sum()\n",
    "        \n",
    "        group_ll = []\n",
    "        \n",
    "        for idx in sample_idx:\n",
    "            mean = mean_flat[idx]\n",
    "            \n",
    "            # Reconstruct Cholesky\n",
    "            L = np.zeros((n_cat_minus_1, n_cat_minus_1))\n",
    "            L[np.tril_indices(n_cat_minus_1)] = chol_flat[idx]\n",
    "            \n",
    "            # Sample random effect for new group\n",
    "            z = np.random.randn(n_cat_minus_1)\n",
    "            re = L @ z\n",
    "            \n",
    "            # Compute probabilities\n",
    "            logits = np.concatenate([[0], mean + re])\n",
    "            probs = softmax(logits)\n",
    "            \n",
    "            # Log-likelihood\n",
    "            ll = multinomial.logpmf(counts, n=total, p=probs)\n",
    "            group_ll.append(ll)\n",
    "        \n",
    "        # Log of mean (not mean of log) for proper averaging\n",
    "        log_liks.append(np.log(np.mean(np.exp(group_ll))))\n",
    "    \n",
    "    return np.sum(log_liks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_re_model(train_data, n_groups):\n",
    "    \"\"\"\n",
    "    Build and fit the random effects multinomial model.\n",
    "    \"\"\"\n",
    "    n_cat = 3\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        group_idx = pm.Data(\"group_idx\", train_data[\"group_idx\"].values)\n",
    "        count_obs = pm.Data(\"count_obs\", train_data[[\"cat_0\", \"cat_1\", \"cat_2\"]].values)\n",
    "        total_obs = pm.Data(\"total_obs\", train_data[\"total\"].values)\n",
    "        \n",
    "        sd_dist = pm.HalfStudentT.dist(nu=3, sigma=2)\n",
    "        chol_cov, _, _ = pm.LKJCholeskyCov(\n",
    "            \"chol_cov\", n=n_cat - 1, eta=1, sd_dist=sd_dist\n",
    "        )\n",
    "        \n",
    "        z = pm.Normal(\"z\", 0, 1, shape=(n_groups, n_cat - 1))\n",
    "        group_effects_m = pm.Deterministic(\"group_effects\", pt.dot(z, chol_cov.T))\n",
    "        \n",
    "        mean_effect = pm.Normal(\"mean_effect\", 0, 1, shape=(n_cat - 1))\n",
    "        \n",
    "        logits = pt.concatenate(\n",
    "            [pt.zeros((n_groups, 1)), group_effects_m + mean_effect],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        p = pm.math.softmax(logits, axis=1)\n",
    "        pm.Multinomial(\"obs\", p=p[group_idx], n=total_obs, observed=count_obs)\n",
    "        \n",
    "        trace = pm.sample(\n",
    "            draws=500, tune=500,  # Reduced for speed in CV\n",
    "            random_seed=42, \n",
    "            target_accept=0.95,\n",
    "            progressbar=False\n",
    "        )\n",
    "    \n",
    "    return model, trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the computational cost of K-fold CV (refitting K times), we'll demonstrate with 3 folds. In practice, 5-10 folds is common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3-fold CV (reduced for demonstration)\n",
    "print(\"Running K-fold CV with group-level folds...\")\n",
    "cv_results = kfold_cv_multinomial(build_re_model, agg_df, n_folds=3, random_seed=42)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"K-Fold CV Results:\")\n",
    "total_elpd = sum(r['test_elpd'] for r in cv_results)\n",
    "print(f\"Total ELPD (sum across folds): {total_elpd:.2f}\")\n",
    "for r in cv_results:\n",
    "    print(f\"  Fold {r['fold']+1}: ELPD = {r['test_elpd']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Model Comparison: Logistic-Normal vs Dirichlet-Multinomial\n",
    "\n",
    "Two common approaches for modeling overdispersed multinomial data:\n",
    "\n",
    "1. **Logistic-Normal Multinomial** (what we've been using): Random effects on the logit scale\n",
    "   - Can model any correlation structure (positive or negative)\n",
    "   - More flexible but more complex\n",
    "\n",
    "2. **Dirichlet-Multinomial**: Random effects on the probability simplex\n",
    "   - Only supports negative correlations between categories\n",
    "   - Simpler and often samples more efficiently\n",
    "   - Appropriate when categories compete (increase in one decreases others)\n",
    "\n",
    "Let's compare both models on our simulated data (which has positive correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dirichlet-Multinomial model\n",
    "with pm.Model() as dm_model:\n",
    "    group_idx = pm.Data(\"group_idx\", agg_df[\"group_id\"].values)\n",
    "    count_obs = pm.Data(\"count_obs\", agg_df[[\"cat_0\", \"cat_1\", \"cat_2\"]].values)\n",
    "    total_obs = pm.Data(\"total_obs\", agg_df[\"total\"].values)\n",
    "    \n",
    "    # Concentration parameter for Dirichlet\n",
    "    concentration = pm.HalfNormal('concentration', sigma=5, shape=3)\n",
    "    \n",
    "    # Group-specific probabilities from Dirichlet\n",
    "    group_alpha = pm.Dirichlet(\n",
    "        'group_alpha', \n",
    "        a=concentration, \n",
    "        shape=(num_groups, 3)\n",
    "    )\n",
    "    \n",
    "    pm.DirichletMultinomial(\n",
    "        'obs', \n",
    "        a=group_alpha[group_idx], \n",
    "        n=total_obs, \n",
    "        observed=count_obs\n",
    "    )\n",
    "    \n",
    "    dm_trace = pm.sample(\n",
    "        random_seed=42, \n",
    "        target_accept=0.95,\n",
    "        idata_kwargs={\"log_likelihood\": True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate posterior predictive samples\n",
    "with dm_model:\n",
    "    dm_ppc = pm.sample_posterior_predictive(dm_trace, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare correlation structures\n",
    "obs_counts = agg_df[[\"cat_0\", \"cat_1\", \"cat_2\"]].values\n",
    "obs_corr = np.corrcoef(obs_counts.T)\n",
    "\n",
    "# Dirichlet-Multinomial predictions\n",
    "dm_pred = az.extract(dm_ppc, group=\"posterior_predictive\", var_names=[\"obs\"]).values\n",
    "dm_means = dm_pred.mean(axis=2)\n",
    "dm_corr = np.corrcoef(dm_means.T)\n",
    "\n",
    "# Logistic-Normal predictions\n",
    "ln_pred = az.extract(re_ppc, group=\"posterior_predictive\", var_names=[\"obs\"]).values\n",
    "ln_means = ln_pred.mean(axis=2)\n",
    "ln_corr = np.corrcoef(ln_means.T)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "categories = [\"cat_0\", \"cat_1\", \"cat_2\"]\n",
    "\n",
    "for ax, corr_mat, title in zip(\n",
    "    axes, \n",
    "    [obs_corr, dm_corr, ln_corr],\n",
    "    ['Observed', 'Dirichlet-Multinomial', 'Logistic-Normal']\n",
    "):\n",
    "    sns.heatmap(\n",
    "        corr_mat, annot=True, fmt='.2f',\n",
    "        xticklabels=categories, yticklabels=categories,\n",
    "        ax=ax, cmap='RdBu_r', vmin=-1, vmax=1, center=0\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.suptitle('Correlation Structure Comparison', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key observation:** The Dirichlet-Multinomial model can only produce negative correlations between categories (due to the constraint that probabilities sum to 1). When the true data has positive correlations (as in our simulation), the Logistic-Normal model is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt LOO comparison (expect warnings)\n",
    "print(\"Attempting LOO comparison (expect Pareto k warnings)...\\n\")\n",
    "\n",
    "loo_ln = az.loo(re_trace)\n",
    "loo_dm = az.loo(dm_trace)\n",
    "\n",
    "print(f\"\\nLogistic-Normal - Bad Pareto k: {np.sum(loo_ln.pareto_k > 0.7)}\")\n",
    "print(f\"Dirichlet-Multinomial - Bad Pareto k: {np.sum(loo_dm.pareto_k > 0.7)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Despite warnings, let's see the comparison\n",
    "comparison = az.compare({\n",
    "    'Logistic-Normal': re_trace, \n",
    "    'Dirichlet-Multinomial': dm_trace\n",
    "})\n",
    "print(comparison[['rank', 'elpd_loo', 'p_loo', 'weight', 'warning']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Given the Pareto k warnings, these LOO estimates should be interpreted with caution. For reliable model comparison, use K-fold CV as demonstrated in Section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Conclusion\n",
    "\n",
    "### Summary\n",
    "\n",
    "We've covered the progression from basic to advanced multinomial regression in PyMC:\n",
    "\n",
    "1. **Basic multinomial regression** can be done elegantly with Bambi for simple cases\n",
    "\n",
    "2. **ZeroSumNormal** provides a modern approach to the identifiability problem, treating categories symmetrically\n",
    "\n",
    "3. **Random effects** require PyMC for full flexibility, especially for correlated random effects using `LKJCholeskyCov`\n",
    "\n",
    "4. **Cross-validation** for hierarchical multinomial models requires special care:\n",
    "   - LOO-PSIS often fails (check Pareto k diagnostics!)\n",
    "   - K-fold CV is more robust\n",
    "   - **Critical:** Use group-level folds, not observation-level\n",
    "\n",
    "5. **Model selection** between Logistic-Normal and Dirichlet-Multinomial depends on the correlation structure in your data\n",
    "\n",
    "### Guidelines: Bambi vs PyMC\n",
    "\n",
    "| Use Bambi when... | Use PyMC when... |\n",
    "|-------------------|------------------|\n",
    "| Simple fixed effects | Correlated random effects |\n",
    "| Quick exploration | Custom likelihood functions |\n",
    "| Standard priors suffice | Need full control over priors |\n",
    "| Interpretable formula syntax | Complex hierarchical structures |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. Always check Pareto k diagnostics when using LOO\n",
    "2. For hierarchical models, split by groups in cross-validation\n",
    "3. Use the non-centered parameterization for random effects\n",
    "4. Consider the true correlation structure when choosing between Logistic-Normal and Dirichlet-Multinomial\n",
    "5. ZeroSumNormal is often preferable to the pivot approach for interpretability\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [PyMC Documentation](https://www.pymc.io/)\n",
    "- [Bambi Documentation](https://bambinos.github.io/bambi/)\n",
    "- [ArviZ Model Comparison](https://python.arviz.org/en/latest/user_guide/model_comparison.html)\n",
    "- Gelman et al., \"Bayesian Data Analysis\" (Chapter 14 on hierarchical models)\n",
    "- McElreath, \"Statistical Rethinking\" (Chapter 12 on multilevel models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
